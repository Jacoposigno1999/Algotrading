{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfbabcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import bs4 as bs\n",
    "import pickle\n",
    "import requests\n",
    "import os\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from collections import Counter\n",
    "from sklearn import svm,neighbors, metrics\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve,auc\n",
    "\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b807d",
   "metadata": {},
   "source": [
    "First of all let's create 2 functions:\n",
    "- Yhe first to retrive the Nasdaq tickers from internet and store the in a list\n",
    "- The second for save in csv file, the stocks' information with respect to each ticket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb4ffa04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already have ATVI\n",
      "Already have ADBE\n",
      "Already have ADP\n",
      "Already have ABNB\n",
      "Already have ALGN\n",
      "Already have GOOGL\n",
      "Already have GOOG\n",
      "Already have AMZN\n",
      "Already have AMD\n",
      "Already have AEP\n",
      "Already have AMGN\n",
      "Already have ADI\n",
      "Already have ANSS\n",
      "Already have AAPL\n",
      "Already have AMAT\n",
      "Already have ASML\n",
      "Already have AZN\n",
      "Already have TEAM\n",
      "Already have ADSK\n",
      "Already have BIDU\n",
      "Already have BIIB\n",
      "Already have BKNG\n",
      "Already have AVGO\n",
      "Already have CDNS\n",
      "Already have CHTR\n",
      "Already have CTAS\n",
      "Already have CSCO\n",
      "Already have CTSH\n",
      "Already have CMCSA\n",
      "Already have CEG\n",
      "Already have CPRT\n",
      "Already have COST\n",
      "Already have CRWD\n",
      "Already have CSX\n",
      "Already have DDOG\n",
      "Already have DXCM\n",
      "Already have DOCU\n",
      "Already have DLTR\n",
      "Already have EBAY\n",
      "Already have EA\n",
      "Already have EXC\n",
      "Already have FAST\n",
      "Already have FISV\n",
      "Already have FTNT\n",
      "Already have GILD\n",
      "Already have HON\n",
      "Already have IDXX\n",
      "Already have ILMN\n",
      "Already have INTC\n",
      "Already have INTU\n",
      "Already have ISRG\n",
      "Already have JD\n",
      "Already have KDP\n",
      "Already have KLAC\n",
      "Already have KHC\n",
      "Already have LRCX\n",
      "Already have LCID\n",
      "Already have LULU\n",
      "Already have MAR\n",
      "Already have MRVL\n",
      "Already have MTCH\n",
      "Already have MELI\n",
      "Already have META\n",
      "Already have MCHP\n",
      "Already have MU\n",
      "Already have MSFT\n",
      "Already have MRNA\n",
      "Already have MDLZ\n",
      "Already have MNST\n",
      "Already have NTES\n",
      "Already have NFLX\n",
      "Already have NVDA\n",
      "Already have NXPI\n",
      "Already have ORLY\n",
      "Already have OKTA\n",
      "Already have ODFL\n",
      "Already have PCAR\n",
      "Already have PANW\n",
      "Already have PAYX\n",
      "Already have PYPL\n",
      "Already have PEP\n",
      "Already have PDD\n",
      "Already have QCOM\n",
      "Already have REGN\n",
      "Already have ROST\n",
      "Already have SGEN\n",
      "Already have SIRI\n",
      "Already have SWKS\n",
      "Already have SPLK\n",
      "Already have SBUX\n",
      "Already have SNPS\n",
      "Already have TMUS\n",
      "Already have TSLA\n",
      "Already have TXN\n",
      "Already have VRSN\n",
      "Already have VRSK\n",
      "Already have VRTX\n",
      "Already have WBA\n",
      "Already have WDAY\n",
      "Already have XEL\n",
      "Already have ZM\n",
      "Already have ZS\n"
     ]
    }
   ],
   "source": [
    "def save_nasdaq_tickers():\n",
    "    resp = requests.get('https://en.wikipedia.org/wiki/Nasdaq-100')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'id': 'constituents'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[1].text\n",
    "        tickers.append(ticker)\n",
    "        \n",
    "    with open(\"nasdaq100_ticket.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers,f)\n",
    "        \n",
    "    return tickers\n",
    "\n",
    "#save_nasdaq_tickers()\n",
    "\n",
    "def get_data_from_yahoo(reload_sp500=False): # Here's where I'll just show a quick example of one way you could handle for \n",
    "                                             # whether or not to reload the S&P 500 list. If we ask it to, the program will \n",
    "                                             # re-pull the S&P 500 list, otherwise it will just use our pickle.\n",
    "    if reload_sp500:\n",
    "        tickers = save_sp500_tickers()\n",
    "    else:\n",
    "        with open(\"nasdaq100_ticket.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n",
    "\n",
    "    start = dt.datetime(2016, 1, 1)\n",
    "    end = dt.datetime.now()\n",
    "    for ticker in tickers:\n",
    "        # just in case your connection breaks, we'd like to save our progress!\n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):\n",
    "            df = web.DataReader(ticker, 'yahoo', start, end)\n",
    "            df.reset_index(inplace=True)\n",
    "            df.set_index(\"Date\", inplace=True)\n",
    "            #df = df.drop(\"Symbol\", axis=1)\n",
    "            df.to_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "\n",
    "            \n",
    "get_data_from_yahoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb6861",
   "metadata": {},
   "source": [
    "Now we want to create an unique dataset that contains adj_close data for all the 100 tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1cecee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ATVI       ADBE        ADP  ABNB       ALGN      GOOGL  \\\n",
      "Date                                                                      \n",
      "2016-01-04  35.976723  91.970001  71.045609   NaN  64.269997  37.972000   \n",
      "2016-01-05  35.517689  92.339996  71.218727   NaN  64.949997  38.076500   \n",
      "2016-01-06  35.182983  91.019997  70.344421   NaN  64.870003  37.966499   \n",
      "2016-01-07  34.685696  89.110001  68.197548   NaN  63.500000  37.049999   \n",
      "2016-01-08  34.150146  87.849998  67.634834   NaN  64.529999  36.545502   \n",
      "\n",
      "                 GOOG       AMZN   AMD        AEP  ...       TSLA        TXN  \\\n",
      "Date                                               ...                         \n",
      "2016-01-04  37.091999  31.849501  2.77  46.350529  ...  14.894000  45.526722   \n",
      "2016-01-05  37.129002  31.689501  2.75  46.731949  ...  14.895333  45.049168   \n",
      "2016-01-06  37.181000  31.632500  2.51  46.906773  ...  14.602667  44.755928   \n",
      "2016-01-07  36.319500  30.396999  2.28  46.366428  ...  14.376667  43.314892   \n",
      "2016-01-08  35.723499  30.352501  2.14  46.294922  ...  14.066667  42.192219   \n",
      "\n",
      "                 VRSN       VRSK        VRTX        WBA       WDAY        XEL  \\\n",
      "Date                                                                            \n",
      "2016-01-04  83.949997  73.467896  122.889999  67.519325  77.760002  29.277351   \n",
      "2016-01-05  81.410004  73.204460  123.449997  65.746994  77.239998  29.572584   \n",
      "2016-01-06  81.019997  72.823936  122.230003  64.738876  74.400002  29.884224   \n",
      "2016-01-07  80.550003  71.262878  114.959999  65.990883  71.750000  29.999031   \n",
      "2016-01-08  78.150002  71.584846  110.709999  65.893333  71.070000  29.670994   \n",
      "\n",
      "            ZM  ZS  \n",
      "Date                \n",
      "2016-01-04 NaN NaN  \n",
      "2016-01-05 NaN NaN  \n",
      "2016-01-06 NaN NaN  \n",
      "2016-01-07 NaN NaN  \n",
      "2016-01-08 NaN NaN  \n",
      "\n",
      "[5 rows x 102 columns]\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\39388\\AppData\\Local\\Temp\\ipykernel_13388\\537361283.py\", line 26, in <cell line: 26>\n",
      "    compile_data()\n",
      "  File \"C:\\Users\\39388\\AppData\\Local\\Temp\\ipykernel_13388\\537361283.py\", line 22, in compile_data\n",
      "    main_df.to_csv('nasdaq_joined_closes.csv')\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 211, in wrapper\n",
      "    return _deprecate_kwarg\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\pandas\\core\\generic.py\", line 3721, in to_csv\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 211, in wrapper\n",
      "    return _deprecate_kwarg\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\pandas\\io\\formats\\format.py\", line 1170, in to_csv\n",
      "    index_label=index_label,\n",
      "TypeError: __init__() got an unexpected keyword argument 'lineterminator'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "def compile_data():\n",
    "    with open(\"nasdaq100_ticket.pickle\",\"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "    for ticker in tickers:\n",
    "        df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        df.set_index('Date', inplace=True)\n",
    "\n",
    "        df.rename(columns={'Adj Close':ticker}, inplace=True) # we are renaming the adj close column with the ticket's name\n",
    "        df.drop(['Open','High','Low','Close','Volume'], axis = 1,inplace=True) # '1' means tha we are dropping columns\n",
    "\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df, how='outer') # outer is usefull for not loose information of all tickers  \n",
    "                                                    # if for some tickers we don't have data for that day\n",
    "\n",
    "\n",
    "    print(main_df.head())\n",
    "    main_df.to_csv('nasdaq_joined_closes.csv')\n",
    "    \n",
    "\n",
    "\n",
    "compile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b1e332",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050453d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_data():\n",
    "    df = pd.read_csv('nasdaq_joined_closes.csv')\n",
    "\n",
    "    df_corr = df.corr(numeric_only = 'TRUE')\n",
    "\n",
    "    data1 = df_corr.values\n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.add_subplot(111) #111 is the size of the plot\n",
    "    heatmap1 = ax1.pcolor(data1, cmap=plt.cm.RdYlGn)\n",
    "    fig1.colorbar(heatmap1)\n",
    "\n",
    "    ax1.set_xticks(np.arange(data1.shape[1]) + 0.5, minor=False)\n",
    "    ax1.set_yticks(np.arange(data1.shape[0]) + 0.5, minor=False)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.xaxis.tick_top()\n",
    "    column_labels = df_corr.columns\n",
    "    row_labels = df_corr.index\n",
    "    ax1.set_xticklabels(column_labels)\n",
    "    ax1.set_yticklabels(row_labels)\n",
    "    plt.xticks(rotation=90)\n",
    "    heatmap1.set_clim(-1, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee7f4a5",
   "metadata": {},
   "source": [
    "## Machine Learnign \n",
    "\n",
    "As featuresets we will take into account all company percent changes that day, and those will be our features. Our label will be whether or not Google(GOOG) rose more than x% within the next x days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13193d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_labels(ticker):\n",
    "    hm_days = 7\n",
    "    df = pd.read_csv('nasdaq_joined_closes.csv', index_col=0)\n",
    "    tickers = df.columns.values.tolist()\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    for i in range(1,hm_days+1):\n",
    "        df['{}_{}d'.format(ticker,i)] = (df[ticker].shift(-i) - df[ticker]) / df[ticker] #df[ticker].shift(-i) is the next day price, basically the row below\n",
    "        df['{}_{}d'.format(ticker,i)] = (df[ticker].shift(-i) - df[ticker]) / df[ticker]\n",
    "\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df, tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118c61e",
   "metadata": {},
   "source": [
    "This function create new columns, in which there are the % change of the price in the last n days, for the selected ticker.\n",
    "So for each row we have, the closing price off the 100 companies that day, and the % change that the intersted firm has experienced in the following n days.\n",
    "\n",
    "Es. GOOG_3d, means that in the next 3 day from that day, google has grown by tot%.\n",
    "\n",
    "Of course for the last days we cant calculate those values, since we don't know the future values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f11f6",
   "metadata": {},
   "source": [
    "Now we ara going to write the function that creates our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "922c2f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buy_sell_hold(*args):\n",
    "    cols = [c for c in args]\n",
    "    requirement = 0.02\n",
    "    for col in cols:\n",
    "        if col > requirement:\n",
    "            return 1\n",
    "        if col < -requirement:\n",
    "            return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e104ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 ATVI        ADBE         ADP        ABNB        ALGN  \\\n",
      "Date                                                                    \n",
      "2022-10-10  73.750000  285.720001  228.690002  110.989998  207.380005   \n",
      "2022-10-11  73.550003  284.829987  226.270004  106.820000  208.449997   \n",
      "2022-10-12  73.500000  286.149994  224.110001  112.349998  208.050003   \n",
      "2022-10-13  73.120003  294.739990  229.899994  112.650002  212.309998   \n",
      "2022-10-14  72.129997  287.940002  225.910004  109.160004  201.630005   \n",
      "\n",
      "                GOOGL       GOOG        AMZN        AMD        AEP  ...  \\\n",
      "Date                                                                ...   \n",
      "2022-10-10  97.860001  98.709999  113.669998  57.810001  84.449997  ...   \n",
      "2022-10-11  97.180000  98.050003  112.209999  57.630001  84.870003  ...   \n",
      "2022-10-12  97.559998  98.300003  112.900002  57.849998  81.739998  ...   \n",
      "2022-10-13  99.059998  99.709999  112.529999  58.939999  84.779999  ...   \n",
      "2022-10-14  96.559998  97.180000  106.900002  55.939999  83.519997  ...   \n",
      "\n",
      "                  XEL         ZM          ZS   GOOG_1d   GOOG_2d   GOOG_3d  \\\n",
      "Date                                                                         \n",
      "2022-10-10  60.209999  73.720001  157.179993 -0.006686 -0.004154  0.010131   \n",
      "2022-10-11  59.830002  73.709999  148.729996  0.002550  0.016930 -0.008873   \n",
      "2022-10-12  57.939999  75.220001  147.630005  0.014344 -0.011394  0.000000   \n",
      "2022-10-13  59.740002  74.730003  145.130005 -0.025374  0.000000  0.000000   \n",
      "2022-10-14  58.860001  72.290001  135.309998  0.000000  0.000000  0.000000   \n",
      "\n",
      "            GOOG_4d  GOOG_5d  GOOG_6d  GOOG_7d  \n",
      "Date                                            \n",
      "2022-10-10  -0.0155      0.0      0.0      0.0  \n",
      "2022-10-11   0.0000      0.0      0.0      0.0  \n",
      "2022-10-12   0.0000      0.0      0.0      0.0  \n",
      "2022-10-13   0.0000      0.0      0.0      0.0  \n",
      "2022-10-14   0.0000      0.0      0.0      0.0  \n",
      "\n",
      "[5 rows x 109 columns]\n",
      "(1709, 109)\n"
     ]
    }
   ],
   "source": [
    "# just to check where we are\n",
    "df,tickers = process_data_for_labels('GOOG')\n",
    "\n",
    "print(df.tail())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf11d4",
   "metadata": {},
   "source": [
    "The following function will let us see the distributions of classes both in our dataset and in our algorithm's predictions. We dont want to feed highly imbalanced datasets to machine learning classifiers, and we also want to see if our classifier is predicting only one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a4f6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_featuresets(ticker):\n",
    "    df, tickers = process_data_for_labels(ticker)\n",
    "    \n",
    "    # The map() function applies a given function to each item of an iterable (list, tuple etc.) and returns an iterator.\n",
    "    # so in this case we are applying the buy_sell_hold function to all the columns that follows\n",
    "\n",
    "    df['{}_target'.format(ticker)] = list(map( buy_sell_hold,\n",
    "                                               df['{}_1d'.format(ticker)],\n",
    "                                               df['{}_2d'.format(ticker)],\n",
    "                                               df['{}_3d'.format(ticker)],\n",
    "                                               df['{}_4d'.format(ticker)],\n",
    "                                               df['{}_5d'.format(ticker)],\n",
    "                                               df['{}_6d'.format(ticker)],\n",
    "                                               df['{}_7d'.format(ticker)] ))\n",
    "    \n",
    "    vals = df['{}_target'.format(ticker)].values.tolist() #is a list with all -1,0,1\n",
    "    print(type(vals))\n",
    "    str_vals = [str(i) for i in vals]\n",
    "    print('Data spread:', Counter(str_vals))\n",
    "    \n",
    "    #clean the data\n",
    "    df.fillna(0, inplace=True)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan) #if we pass from 0 to any value, the % change will be +inf \n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    #right now our \"features\" are that day's prices for stocks. Just static numbers, really nothing telling at all. \n",
    "    # What we want company's percent change that day. \n",
    "    df_vals = df[[ticker for ticker in tickers]].pct_change() #by deafualt the shift for which is calculated the % change is 1\n",
    "\n",
    "    df_vals = df_vals.replace([np.inf, -np.inf], 0)\n",
    "    df_vals.fillna(0, inplace=True)\n",
    "    \n",
    "    X = df_vals.values\n",
    "    y = df['{}_target'.format(ticker)].values\n",
    "    return X, y, df\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb83647",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> PROBABILMENTE ORA COME ORA BUY_SELD_HOLD VIENE APPLICATO SOLO ALLA PRIMA COLLANNA CHE LI VIENE PASSAT,\n",
    "E QUINDI IN QUESTO CASO AL PREZZO SUCCESSIVO DI UN GIORNO, NON SONO SICURO DI QUESTA COSA, ORA COME ORA PRENDILA COSÃ¬ </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf19524d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Data spread: Counter({'1': 771, '-1': 575, '0': 363})\n"
     ]
    }
   ],
   "source": [
    "#just to check \n",
    "X,y,df = extract_featuresets('GOOG')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ff18d",
   "metadata": {},
   "source": [
    "Now we have created a function that gives back what we want as featuresets and labels, we can now use machine learning algorithms with the hope that they will learn to map relationships of existing price changes to future price changes for a company.\n",
    "\n",
    "We'll try to make prediction using 3 differents ML algorithms: \n",
    "- Supported vector machine\n",
    "- K Nearest Neighbors\n",
    "- Random forest classifier\n",
    "\n",
    "Combining them with a *voting classifiere*\n",
    "\n",
    "Voting classifier simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting. The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63063854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the confusion matrix\n",
    "def plot_confusion_matrix(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Create a dataframe with the confussion matrix values\n",
    "    df_cm = pd.DataFrame(cm, range(cm.shape[0]),range(cm.shape[1]))\n",
    "    #plt.figure(figsize = (10,7))\n",
    "    # Plot the confussion matrix\n",
    "    sn.set(font_scale=1.4) #for label size\n",
    "    sn.heatmap(df_cm, annot=True,fmt='.0f',annot_kws={\"size\": 10})# font size\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def do_ml(ticker):\n",
    "    X, y, df = extract_featuresets(ticker)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "    clf = VotingClassifier([('lsvc', svm.LinearSVC()),\n",
    "                            ('knn', neighbors.KNeighborsClassifier()),\n",
    "                            ('rfor', RandomForestClassifier())])\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    confidence = clf.score(X_test, y_test) # with this we already perform prediction and compute measurements like accuracy, ecc..\n",
    "    print('accuracy:', confidence)\n",
    "    predictions = clf.predict(X_test) # we also do prediction in this way to see if our model is only classifying one class,\n",
    "                                      # which is something that can easily happen.\n",
    "    cm = confusion_matrix(y_test,predictions)\n",
    "    print('predicted class counts:', Counter(predictions))\n",
    "    print()\n",
    "    print()\n",
    "    return confidence,y_test,predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "572b0b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Data spread: Counter({'1': 870, '-1': 578, '0': 261})\n",
      "accuracy: 0.4883177570093458\n",
      "predicted class counts: Counter({1: 387, -1: 41})\n",
      "[[ 14   0 124]\n",
      " [  3   0  68]\n",
      " [ 24   0 195]]\n",
      "\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\39388\\AppData\\Local\\Temp\\ipykernel_13388\\232327521.py\", line 2, in <cell line: 2>\n",
      "    plot_confusion_matrix(y_test,predictiosn)\n",
      "  File \"C:\\Users\\39388\\AppData\\Local\\Temp\\ipykernel_13388\\1839283162.py\", line 10, in plot_confusion_matrix\n",
      "    sn.heatmap(df_cm, annot=True,fmt='.0f',annot_kws={\"size\": 10})# font size\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\seaborn\\matrix.py\", line 553, in heatmap\n",
      "    ax = plt.gca()\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\matplotlib\\pyplot.py\", line 2227, in gca\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\matplotlib\\pyplot.py\", line 832, in gcf\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\matplotlib\\_api\\deprecation.py\", line 454, in wrapper\n",
      "    warn_deprecated(\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\matplotlib\\pyplot.py\", line 773, in figure\n",
      "    if num.canvas.manager is None:\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\matplotlib\\pyplot.py\", line 348, in new_figure_manager\n",
      "    Parameters\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\matplotlib\\pyplot.py\", line 338, in _warn_if_gui_out_of_main_thread\n",
      "    the interactive mode takes care of this.\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\matplotlib\\pyplot.py\", line 207, in _get_backend_mod\n",
      "    def _get_backend_mod():\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\matplotlib\\pyplot.py\", line 265, in switch_backend\n",
      "    # are of worse quality.\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 972, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\matplotlib_inline\\__init__.py\", line 1, in <module>\n",
      "    from . import backend_inline, config  # noqa\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\matplotlib_inline\\backend_inline.py\", line 8, in <module>\n",
      "    from matplotlib.backends import backend_agg\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\", line 37, in <module>\n",
      "    from matplotlib.backend_bases import (\n",
      "ImportError: cannot import name '_check_savefig_extra_args' from 'matplotlib.backend_bases' (C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\matplotlib\\backend_bases.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\39388\\anaconda3\\envs\\Finance\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "confidence,y_test,predictiosn = do_ml('AAPL')\n",
    "plot_confusion_matrix(y_test,predictiosn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
